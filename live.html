<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Gemini Live Audio (Strict Translation)</title>
    <style>
        body { font-family: sans-serif; margin: 20px; max-width: 800px; }
        code { background-color: #eee; padding: 2px 4px; border-radius: 4px; }
        #log { margin-top: 15px; border: 1px solid #ccc; padding: 10px; max-height: 300px; overflow-y: scroll; background-color: #f9f9f9; }
        .message { margin-bottom: 5px; }
        .user { color: blue; }
        .system { color: green; }
        .error { color: red; font-weight: bold; }
        #connectionStatus { font-weight: bold; }
        .controls { display: flex; gap: 10px; margin-bottom: 10px; }
    </style>
</head>
<body>

    <h1>Gemini Live Audio Agent</h1>
    <p>Model: <code>models/gemini-2.5-flash-native-audio-preview-09-2025</code></p>

    <div class="controls">
        <label for="apiKey">API Key:</label>
        <input type="password" id="apiKey" value="YOUR_HARDCODED_API_KEY_HERE" style="flex-grow: 1;">
        <button id="connectButton">Connect</button>
        <button id="disconnectButton" disabled>Disconnect</button>
    </div>
    <p><strong>Status:</strong> <span id="connectionStatus" style="color: red;">Disconnected</span></p>

    <hr>

    <h2>Conversation</h2>
    <div class="controls">
        <input type="text" id="userInput" placeholder="Ask Gemini a question (e.g., Explain the speed of light)" style="flex-grow: 1;" disabled>
        <button id="sendButton" disabled>Send</button>
    </div>

    <audio id="audioPlayer" controls hidden></audio>

    <div id="log">
        <div class="system message">Ready. Replace 'YOUR_HARDCODED_API_KEY_HERE' and click Connect.</div>
    </div>

    <script>
        // --- 1. CONFIGURATION ---
        const API_KEY_INPUT = document.getElementById('apiKey');
        // The official model string from your code
        const MODEL_ID_PATH = 'models/gemini-2.5-flash-native-audio-preview-09-2025';
        const LIVE_API_URL = 'wss://live.gemini.google.com/v1/sessions';

        let ws = null;
        let audioParts = []; // Stores Base64 audio chunks
        let audioMimeType = '';

        // --- 2. DOM ELEMENTS ---
        const log = document.getElementById('log');
        const connectButton = document.getElementById('connectButton');
        const disconnectButton = document.getElementById('disconnectButton');
        const userInput = document.getElementById('userInput');
        const sendButton = document.getElementById('sendButton');
        const audioPlayer = document.getElementById('audioPlayer');
        const connectionStatus = document.getElementById('connectionStatus');

        // --- 3. UTILITY FUNCTIONS (Node.js Logic Translated) ---

        // Node.js Buffer is replaced by DataView/Uint8Array for browser compatibility
        function createWavHeader(dataLength, options) {
            // Your original options structure
            const { numChannels, sampleRate, bitsPerSample } = options;

            // Wave format calculations
            const byteRate = sampleRate * numChannels * bitsPerSample / 8;
            const blockAlign = numChannels * bitsPerSample / 8;
            const buffer = new ArrayBuffer(44);
            const view = new DataView(buffer);

            // Helper function to write a string to the buffer
            function writeString(view, offset, string) {
                for (let i = 0; i < string.length; i++) {
                    view.setUint8(offset + i, string.charCodeAt(i));
                }
            }

            // RIFF chunk
            writeString(view, 0, 'RIFF');               // ChunkID
            view.setUint32(4, 36 + dataLength, true);   // ChunkSize
            writeString(view, 8, 'WAVE');               // Format

            // FMT subchunk
            writeString(view, 12, 'fmt ');               // Subchunk1ID
            view.setUint32(16, 16, true);               // Subchunk1Size (PCM)
            view.setUint16(20, 1, true);                // AudioFormat (1 = PCM)
            view.setUint16(22, numChannels, true);      // NumChannels
            view.setUint32(24, sampleRate, true);       // SampleRate
            view.setUint32(28, byteRate, true);         // ByteRate
            view.setUint16(32, blockAlign, true);       // BlockAlign
            view.setUint16(34, bitsPerSample, true);    // BitsPerSample

            // DATA subchunk
            writeString(view, 36, 'data');               // Subchunk2ID
            view.setUint32(40, dataLength, true);       // Subchunk2Size

            return buffer; // Returns ArrayBuffer
        }

        function parseMimeType(mimeType) {
            const [fileType, ...params] = mimeType.split(';').map(s => s.trim());

            const options = {
                numChannels: 1,
                bitsPerSample: 16,
                sampleRate: 24000, // Default for Gemini output audio
            };

            for (const param of params) {
                const [key, value] = param.split('=').map(s => s.trim());
                if (key === 'rate' && !isNaN(parseInt(value))) {
                    options.sampleRate = parseInt(value, 10);
                }
            }
            return options;
        }

        function convertToWav(base64Data, mimeType) {
            const options = parseMimeType(mimeType);

            // 1. Convert all Base64 chunks to raw ArrayBuffers and calculate total length
            const dataBuffers = base64Data.map(data => {
                const binaryString = atob(data);
                const len = binaryString.length;
                const bytes = new Uint8Array(len);
                for (let i = 0; i < len; i++) {
                    bytes[i] = binaryString.charCodeAt(i);
                }
                return bytes.buffer;
            });

            const dataLength = dataBuffers.reduce((a, b) => a + b.byteLength, 0);

            // 2. Create the WAV header
            const wavHeader = createWavHeader(dataLength, options);

            // 3. Concatenate Header and Data
            const combinedBuffer = new Uint8Array(wavHeader.byteLength + dataLength);
            combinedBuffer.set(new Uint8Array(wavHeader), 0);

            let offset = wavHeader.byteLength;
            for (const buffer of dataBuffers) {
                combinedBuffer.set(new Uint8Array(buffer), offset);
                offset += buffer.byteLength;
            }

            // 4. Create and return the Blob for browser playback
            return new Blob([combinedBuffer.buffer], { type: 'audio/wav' });
        }


        // --- 4. UI/HELPER LOGIC ---
        function appendToLog(text, className = 'system') {
            const entry = document.createElement('div');
            entry.className = `message ${className}`;
            entry.textContent = `[${new Date().toLocaleTimeString()}] ${text}`;
            log.appendChild(entry);
            log.scrollTop = log.scrollHeight;
        }

        function setControlsState(isConnected) {
            connectButton.disabled = isConnected;
            disconnectButton.disabled = !isConnected;
            userInput.disabled = !isConnected;
            sendButton.disabled = !isConnected;
            API_KEY_INPUT.disabled = isConnected;
            connectionStatus.textContent = isConnected ? 'Connected' : 'Disconnected';
            connectionStatus.style.color = isConnected ? 'green' : 'red';
        }

        // --- 5. WEBSOCKET CONNECTION ---
        connectButton.onclick = () => {
            const apiKey = API_KEY_INPUT.value.trim();
            if (!apiKey || apiKey === 'YOUR_HARDCODED_API_KEY_HERE') {
                appendToLog("Please enter or replace the placeholder with your API Key.", 'error');
                return;
            }

            if (ws) ws.close();

            appendToLog("Attempting to connect...", 'system');

            const connectionUrl = `${LIVE_API_URL}?key=${apiKey}`;
            ws = new WebSocket(connectionUrl);

            ws.onopen = () => {
                appendToLog("Connection opened.", 'system');
                setControlsState(true);

                // Send the session configuration (BidiGenerateContentSetup)
                const configMessage = {
                    "liveConnectConfig": {
                        "model": MODEL_ID_PATH,
                        "liveGenerationConfig": {
                            // Your official code sets responseModalities: [Modality.AUDIO]
                            "responseModalities": ["AUDIO"],
                            "speechConfig": {
                                "voiceConfig": {
                                    "prebuiltVoiceConfig": {
                                        // Using 'Zephyr' from your official code example
                                        "voiceName": "Zephyr",
                                    }
                                }
                            },
                        },
                        // Context window compression configuration is omitted for simplicity
                        // as it relates to session management, not the connection itself.
                    }
                };
                ws.send(JSON.stringify(configMessage));
                appendToLog(`Sending config for model: ${MODEL_ID_PATH}`, 'system');
            };

            ws.onmessage = (event) => {
                const message = JSON.parse(event.data);

                if (message.error) {
                    appendToLog(`API Error: ${message.error.message}`, 'error');
                    ws.close();
                    return;
                }

                if (message.serverContent) {
                    const parts = message.serverContent?.modelTurn?.parts;

                    if (parts) {
                        for (const part of parts) {
                            if (part.inlineData && part.inlineData.mimeType.startsWith('audio/')) {
                                audioMimeType = part.inlineData.mimeType;
                                audioParts.push(part.inlineData.data);
                            }
                            if(part.text) {
                                // This handles the text transcript you would process in Node.js
                                appendToLog(`Model Text: ${part.text}`, 'system');
                            }
                            // FileData is ignored as the browser cannot write to the filesystem directly
                        }
                    }

                    // Handles the turnComplete signal from the server
                    if (message.serverContent.turnComplete) {
                        appendToLog("Turn complete. Processing audio...", 'system');
                        if (audioParts.length > 0) {
                            // 1. Convert all Base64 chunks to a single WAV Blob (using your translated logic)
                            const audioBlob = convertToWav(audioParts, audioMimeType);

                            // 2. Play the audio in the browser
                            audioPlayer.src = URL.createObjectURL(audioBlob);
                            audioPlayer.hidden = false;
                            audioPlayer.play();
                            appendToLog(`Audio playback started.`, 'system');
                        }

                        // Reset for the next turn
                        audioParts = [];
                        audioMimeType = '';
                    }
                }
            };

            ws.onclose = (event) => {
                appendToLog(`Connection closed: Code ${event.code}. Reason: ${event.reason || 'Unknown'}`, 'error');
                setControlsState(false);
            };

            ws.onerror = (error) => {
                appendToLog(`WebSocket Error occurred. See console for details.`, 'error');
                setControlsState(false);
            };
        };

        // --- 6. USER INPUT ---
        disconnectButton.onclick = () => {
            if (ws) ws.close();
        };

        sendButton.onclick = () => {
            const text = userInput.value.trim();
            if (!ws || ws.readyState !== WebSocket.OPEN || !text) {
                appendToLog("Not connected or empty input.", 'error');
                return;
            }

            // This sends the clientContent turn (session.sendClientContent in Node.js)
            const message = {
                "clientContent": {
                    "turns": [
                        // The string message from your Node.js example
                        text
                    ],
                    "turnComplete": true
                }
            };

            ws.send(JSON.stringify(message));
            appendToLog(`User: ${text}`, 'user');
            userInput.value = '';
            audioPlayer.hidden = true;
        };

    </script>
</body>
</html>