import { useState, useRef, useCallback } from 'react';
import { GoogleGenAI, Modality, LiveSession, LiveServerMessage, Blob } from '@google/genai';
import type { AppStatus, ConversationTurn } from '../types';
import { encode, decode, decodeAudioData } from '../utils/audio';

const INPUT_SAMPLE_RATE = 16000;
const OUTPUT_SAMPLE_RATE = 24000;
const SCRIPT_PROCESSOR_BUFFER_SIZE = 4096;

export const useGeminiLive = () => {
  const [status, setStatus] = useState<AppStatus>('idle');
  const [transcript, setTranscript] = useState<ConversationTurn[]>([]);
  const [error, setError] = useState<string | null>(null);

  const sessionPromiseRef = useRef<Promise<LiveSession> | null>(null);
  const inputAudioContextRef = useRef<AudioContext | null>(null);
  const outputAudioContextRef = useRef<AudioContext | null>(null);
  const scriptProcessorRef = useRef<ScriptProcessorNode | null>(null);
  const mediaStreamRef = useRef<MediaStream | null>(null);
  const nextStartTimeRef = useRef<number>(0);
  const audioSourcesRef = useRef<Set<AudioBufferSourceNode>>(new Set());

  const currentInputTranscriptionRef = useRef('');
  const currentOutputTranscriptionRef = useRef('');

  const aiRef = useRef<GoogleGenAI | null>(null);

  const updateTranscript = (speaker: 'user' | 'model', text: string, isFinal: boolean) => {
    setTranscript(prev => {
      const newTranscript = [...prev];
      const lastTurn = newTranscript[newTranscript.length - 1];

      if (lastTurn && lastTurn.speaker === speaker && !lastTurn.isFinal) {
        lastTurn.text = text;
        lastTurn.isFinal = isFinal;
      } else {
        newTranscript.push({ speaker, text, isFinal });
      }
      return newTranscript;
    });
  };

  const cleanup = useCallback(() => {
    scriptProcessorRef.current?.disconnect();
    scriptProcessorRef.current = null;

    mediaStreamRef.current?.getTracks().forEach(track => track.stop());
    mediaStreamRef.current = null;

    inputAudioContextRef.current?.close().catch(console.error);
    inputAudioContextRef.current = null;

    outputAudioContextRef.current?.close().catch(console.error);
    outputAudioContextRef.current = null;

    audioSourcesRef.current.forEach(source => source.stop());
    audioSourcesRef.current.clear();

    sessionPromiseRef.current = null;
    setStatus('idle');
  }, []);


  const stop = useCallback(async () => {
    if (sessionPromiseRef.current) {
        try {
            const session = await sessionPromiseRef.current;
            session.close();
        } catch (e) {
            console.error('Error closing session:', e);
        }
    }
    cleanup();
  }, [cleanup]);


  const start = useCallback(async () => {
    setError(null);
    setTranscript([]);
    setStatus('connecting');

    if (!process.env.API_KEY) {
      setError("API key is not configured. Please set the API_KEY environment variable.");
      setStatus('error');
      return;
    }

    aiRef.current = new GoogleGenAI({ apiKey: process.env.API_KEY });

    try {
      mediaStreamRef.current = await navigator.mediaDevices.getUserMedia({ audio: true });

      // FIX: Use `(window as any)` to access `webkitAudioContext` to avoid TypeScript type errors for vendor-prefixed APIs.
      inputAudioContextRef.current = new (window.AudioContext || (window as any).webkitAudioContext)({ sampleRate: INPUT_SAMPLE_RATE });
      outputAudioContextRef.current = new (window.AudioContext || (window as any).webkitAudioContext)({ sampleRate: OUTPUT_SAMPLE_RATE });

      sessionPromiseRef.current = aiRef.current.live.connect({
        model: 'gemini-2.5-flash-native-audio-preview-09-2025',
        config: {
          responseModalities: [Modality.AUDIO],
          speechConfig: {
            voiceConfig: { prebuiltVoiceConfig: { voiceName: 'Zephyr' } },
          },
          inputAudioTranscription: {},
          outputAudioTranscription: {},
        },
        callbacks: {
          onopen: () => {
            const source = inputAudioContextRef.current!.createMediaStreamSource(mediaStreamRef.current!);
            scriptProcessorRef.current = inputAudioContextRef.current!.createScriptProcessor(SCRIPT_PROCESSOR_BUFFER_SIZE, 1, 1);

            scriptProcessorRef.current.onaudioprocess = (audioProcessingEvent) => {
              const inputData = audioProcessingEvent.inputBuffer.getChannelData(0);
              const l = inputData.length;
              const int16 = new Int16Array(l);
              for (let i = 0; i < l; i++) {
                int16[i] = inputData[i] * 32768;
              }
              const pcmBlob: Blob = {
                data: encode(new Uint8Array(int16.buffer)),
                mimeType: `audio/pcm;rate=${INPUT_SAMPLE_RATE}`,
              };

              sessionPromiseRef.current?.then((session) => {
                session.sendRealtimeInput({ media: pcmBlob });
              }).catch(e => console.error("Failed to send audio:", e));
            };

            source.connect(scriptProcessorRef.current);
            scriptProcessorRef.current.connect(inputAudioContextRef.current!.destination);
            setStatus('listening');
          },
          onmessage: async (message: LiveServerMessage) => {
            if (message.serverContent?.inputTranscription) {
              const text = message.serverContent.inputTranscription.text;
              currentInputTranscriptionRef.current += text;
              updateTranscript('user', currentInputTranscriptionRef.current, false);
            }
             if (message.serverContent?.outputTranscription) {
              if (status !== 'speaking') setStatus('speaking');
              const text = message.serverContent.outputTranscription.text;
              currentOutputTranscriptionRef.current += text;
              updateTranscript('model', currentOutputTranscriptionRef.current, false);
            }

            if (message.serverContent?.turnComplete) {
              if (currentInputTranscriptionRef.current.trim()) {
                  updateTranscript('user', currentInputTranscriptionRef.current, true);
              }
              if (currentOutputTranscriptionRef.current.trim()) {
                  updateTranscript('model', currentOutputTranscriptionRef.current, true);
              }
              currentInputTranscriptionRef.current = '';
              currentOutputTranscriptionRef.current = '';
              setStatus('listening');
            }

            const base64Audio = message.serverContent?.modelTurn?.parts[0]?.inlineData?.data;
            if (base64Audio && outputAudioContextRef.current) {
              if (status !== 'speaking') setStatus('speaking');
              nextStartTimeRef.current = Math.max(nextStartTimeRef.current, outputAudioContextRef.current.currentTime);
              const audioBuffer = await decodeAudioData(decode(base64Audio), outputAudioContextRef.current, OUTPUT_SAMPLE_RATE, 1);
              const source = outputAudioContextRef.current.createBufferSource();
              source.buffer = audioBuffer;
              source.connect(outputAudioContextRef.current.destination);
              source.start(nextStartTimeRef.current);
              nextStartTimeRef.current += audioBuffer.duration;

              audioSourcesRef.current.add(source);
              source.onended = () => {
                audioSourcesRef.current.delete(source);
              };
            }

            if(message.serverContent?.interrupted){
                audioSourcesRef.current.forEach(source => source.stop());
                audioSourcesRef.current.clear();
                nextStartTimeRef.current = 0;
            }
          },
          onerror: (e) => {
            console.error(e);
            setError(`An error occurred: ${e.message || 'Unknown error'}`);
            setStatus('error');
            cleanup();
          },
          onclose: () => {
            cleanup();
          },
        },
      });
    } catch (e: any) {
      console.error(e);
      setError(`Failed to start microphone: ${e.message}`);
      setStatus('error');
      cleanup();
    }
  }, [cleanup, status]);

  return { status, transcript, error, start, stop };
};
